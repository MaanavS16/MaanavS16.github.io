<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-05-30T03:52:48-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Maanav Singh</title><subtitle>Maanav Singh&apos;s Personal Website</subtitle><author><name>Maanav Singh</name><email>msingh2@unc.edu</email></author><entry><title type="html">Predicting effects of CPU Cache on Application Performance</title><link href="http://localhost:4000/systems/embedded/2023/05/29/cache-exploration.html" rel="alternate" type="text/html" title="Predicting effects of CPU Cache on Application Performance" /><published>2023-05-29T15:23:54-04:00</published><updated>2023-05-29T15:23:54-04:00</updated><id>http://localhost:4000/systems/embedded/2023/05/29/cache-exploration</id><content type="html" xml:base="http://localhost:4000/systems/embedded/2023/05/29/cache-exploration.html">&lt;p&gt;Memory access times play a huge role on performance characteristics of programs. Here I conduct a set of experiments to isolate and understand these effects.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Memory is absolutely essential in the field of computing. From an automata theory standpoint, it’s the key tool that enables us to reason with context-free and recursively enumerable languages. Beyond the theoretical power memory yields, the behavior of various memory models and techniques heavily influence modern software.&lt;/p&gt;

&lt;p&gt;My goal with these experiments is to fill in some of the holes in my knowledge with some hands on analysis and some documentation I can reference in the future. Before diving into the details I’d like to do a bottom-up review of computer memory in general.&lt;/p&gt;

&lt;h2 id=&quot;types-of-memory&quot;&gt;Types of Memory&lt;/h2&gt;
&lt;p&gt;The basic concept of memory in computer is to create a physical representation for data that can be written to and read from. Pretty simple concept, but the physical design and fabrication is incredibly complicated. The most common forms of memory that you’ll encounter while working with computers are listed in the following table.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Access Latency&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Static Random Access Memory (SRAM)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;1-10 ns&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Super fast and super expensive. Built with transistors + pretty much a &lt;a href=&quot;https://www.build-electronic-circuits.com/d-latch/&quot;&gt;D-Latch&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dynamic Random Access Memory (DRAM)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;~50 ns&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Main memory implemented with this. Fast and expensive&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Flash Memory&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;1-100 µs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;No moving components and still non-volatile&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Disk Storage&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;~10 ms&lt;/strong&gt; w/ 7200 rpm&lt;/td&gt;
      &lt;td&gt;Cheap spinning disks with magnetic platters and a read/write head that sets and specific magnetized regions.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Magnetic Tape&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;seconds to hours&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Used for archival storage. Extremely high capacity per unit cost. Cool &lt;a href=&quot;https://archiveprogram.github.com/arctic-vault/&quot;&gt;example&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p class=&quot;notice&quot;&gt;Note: All of these access times are mostly for determining a general order of magnitude. At small-time scales the distance between the processor and the memory cells can play a significant role in access latency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MemoryHierarchy.png&quot; alt=&quot;memory hierarchy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above figure demonstrates the scarcity, speed, and cost of the memory types. Where the top of the pyramid has the least quantity, highest cost, and highest speed.&lt;/p&gt;

&lt;h2 id=&quot;cache-layout&quot;&gt;Cache Layout:&lt;/h2&gt;

&lt;p&gt;The Von Neumann architecture prescribes a unified memory store that both contains instructions and application data. While this is mostly true, as the system memory is unified, there exists a lower latency intermediate memory within the CPU that contains a portion of the memory space called the Cache. The CPU can often avoid making costly accesses to the main memory by taking advantage of this. The cache saves data entries that are likely to be accessed in the future and makes them available for quick access. When a particular memory addresses is trying to resolved, the cache is first checked and can potentially massively decrease access latency.&lt;/p&gt;

&lt;p&gt;This cache differs from the main memory in more ways than just speed however. The cache is designed to determine what’s “likely to be accessed in the future”. The strategy for completing this determination varies between caches and depends on the access patterns and configuration.&lt;/p&gt;

&lt;p&gt;There are two guiding observations that determine the high level design of caches.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If an address is accessed, then adjacent elements are also likely to be accessed (&lt;a href=&quot;https://en.wikipedia.org/wiki/Locality_of_reference&quot;&gt;Spatial Locality&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;If an address is accessed, then that element is likely to be accessed in the future (&lt;a href=&quot;https://en.wikipedia.org/wiki/Locality_of_reference&quot;&gt;Temporal Locality&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I won’t go into much detail beyond these fairly straightforward definitions, but the influence of these two observations should hopefully be evident moving forward.&lt;/p&gt;

&lt;h3 id=&quot;should-instructions-and-data-be-cached-separately&quot;&gt;Should instructions and data be cached separately?&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Decision&lt;/th&gt;
      &lt;th&gt;Pros&lt;/th&gt;
      &lt;th&gt;Cons&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;More hits per cache = more hits overall&lt;/td&gt;
      &lt;td&gt;If predecessor cache’s use a separate store, then then the access patterns will make this less cache effective.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;Simple and fast&lt;/td&gt;
      &lt;td&gt;Cache trashing can occur: lines are being wasted by flip-flopping between instructions and data&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Due to the advantages of separately caching instructions and data, most modern processors have separate caches at the first level. However, subsequent layers are unified due to the diminishing performance returns with further separation on larger caches. The following diagram diagrams the cache in my current CPU. Notably, there are 3 stages to the cache.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L1i: Per core instruction cache + L1d: Per core data cache&lt;/li&gt;
  &lt;li&gt;L2: Per core unified cache following L1&lt;/li&gt;
  &lt;li&gt;L3: Across core unified cache&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cpu_diagram.png&quot; alt=&quot;cpu diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that we’ve covered the high level design of a cpu cache scheme, let’s focus on the specific design of each of the cache boxes in the diagram. There are 2 main types of cache strategies: Direct map and Set associative.&lt;/p&gt;

&lt;h3 id=&quot;direct-mapped-or-set-associative&quot;&gt;Direct mapped or set associative?&lt;/h3&gt;
&lt;h4 id=&quot;direct-mapped&quot;&gt;Direct mapped:&lt;/h4&gt;

&lt;p&gt;Partition the memory space into n regions, where there are n lines in the cache (So each partition gets a single line in the physical cache). If n is a power of two, then you can use the upper-order bits of an address to identify what the corresponding cache line is.&lt;/p&gt;

&lt;p&gt;Each cache contains multiple words, meaning a single memory access&lt;/p&gt;

&lt;p&gt;Steps for memory lookup in a direct mapped cache:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CPU executes load instruction for a particular address.&lt;/li&gt;
  &lt;li&gt;Set index portion of the address determines the relevant cache line&lt;/li&gt;
  &lt;li&gt;(a) if the valid flag is set and the tag matches then the cache has hit. Return correct word by finding the offset within the data-field.&lt;/li&gt;
  &lt;li&gt;(b) otherwise, issue a memory fetch from upper level cache/memory. After that’s resolved, evict the data and update the data field, tag, and set the valid bit.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Block-diagram-of-a-direct-mapped-cache.png&quot; alt=&quot;dmcache&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;set-associative&quot;&gt;Set Associative:&lt;/h4&gt;

&lt;p&gt;A set associative cache is similar to a stack of smaller direct mapped caches. That’s to say, each partition of the memory space has k corresponding caches lines across the k stacked direct mapped caches. This type of cache is called a k-way set associative cache.&lt;/p&gt;

&lt;p&gt;Now that there are more than one cache lines for a particular set, more logic needs to be implemented for deciding which entires from the set get to stay and which get evicted from the cache. In order to implement this behavior, there are a few bits in the cache line to store information about that particular entry.&lt;/p&gt;

&lt;p&gt;These bits are often used to store either frequency or access time information. With that the eviction strategy can be to remove the least frequently accessed data region or the last accessed data region. (LFU/LRU caches)&lt;/p&gt;

&lt;p&gt;Steps for memory lookup in a direct mapped cache:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CPU executes load instruction for a particular address.&lt;/li&gt;
  &lt;li&gt;Set index portion of the address determines the relevant cache lines&lt;/li&gt;
  &lt;li&gt;(a) If the valid flag is set and the tag matches in any of the cache lines, return correct word by finding the offset within the data-field. Update the frequency or access time bits if applicable&lt;/li&gt;
  &lt;li&gt;(b) otherwise, issue a memory fetch from upper level cache/memory. After, determine which line should be evicted to make space if any.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sacache.jpg&quot; alt=&quot;sacache&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A set associative cache is more advanced than a direct map cache, which results in more intelligent cache eviction and consequently higher hit rates. However, this comes at the cost of complexity and it’s associated latency increases.&lt;/p&gt;

&lt;p&gt;average_latency = hit% * (cache access latency) + (1-hit%) * (memory access latency)&lt;/p&gt;

&lt;p&gt;Where, hit% is determined by the cache type, strategy, and access patterns
and cache access latency is determined by the design and complexity of the cache.&lt;/p&gt;

&lt;p&gt;This overview of the operating of direct map and set associative caches neglects to review the procedure for writing to memory, since the focus on the following sections is read performance; however, it’s important to understand there are mechanisms for marking cache entries as unusable or “dirty” after they’ve been overwritten or modified. There are also challenges with &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_coherence&quot;&gt;Cache Coherence&lt;/a&gt; or making sure that all caches contain correct data even across different CPU cores. This won’t be explored here, but is definitely worth reading into.&lt;/p&gt;

&lt;h2 id=&quot;jumping-into-some-code&quot;&gt;Jumping into some code&lt;/h2&gt;

&lt;p&gt;After the introduction to basic cache structures, I’ve set 2 tasks for myself.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Try to make a perfect situation for the cache to shine in terms of performance.&lt;/li&gt;
  &lt;li&gt;Explore the impact of a bad access pattern on performance, and try to model and predict that impact.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;notice&quot;&gt;
&lt;p&gt;All following benchmarks will be run w/&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;i9-10980XE processor&lt;/li&gt;
  &lt;li&gt;2x32 GB DDR4 Memory @ 2666 MHZ&lt;/li&gt;
  &lt;li&gt;gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/google/benchmark&quot;&gt;google benchmarking library&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h3 id=&quot;c-stl-containers&quot;&gt;C++ STL Containers&lt;/h3&gt;

&lt;p&gt;I will be using various C++ standard STL containers to create a key-value store as my case study for cache performance. In a basic Data Structures setting, performance considerations for an algorithm are solely based on the asymptotic runtime behavior. This is shown to be a very simplistic model and doesn’t convey much information about the runtime characteristics related to the host architecture.&lt;/p&gt;

&lt;p&gt;In my tests, I will measure the lookup time for a particular integer key value pair across &lt;code class=&quot;highlighter-rouge&quot;&gt;std::vector&amp;lt;std::pair&amp;lt;int, int&amp;gt;&amp;gt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;std::map&amp;lt;int, int&amp;gt;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;std::unordered_map&amp;lt;int, int&amp;gt;&lt;/code&gt;. Each of these collections will be initialized with ascending keys and a matching 32 bit integer (8 bytes per element).&lt;/p&gt;

&lt;p&gt;As a quick reminder, these are the following best, average, and worst case lookup algorithm time complexities.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Algorithm&lt;/th&gt;
      &lt;th&gt;Best&lt;/th&gt;
      &lt;th&gt;Average&lt;/th&gt;
      &lt;th&gt;Worst&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;std::unordered_map::find&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;O(1)&lt;/td&gt;
      &lt;td&gt;O(1)&lt;/td&gt;
      &lt;td&gt;O(1)&lt;/td&gt;
      &lt;td&gt;This is a hashmap&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;std::map::find&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;O(log(n))&lt;/td&gt;
      &lt;td&gt;O(log(n))&lt;/td&gt;
      &lt;td&gt;O(log(n))&lt;/td&gt;
      &lt;td&gt;This is a treemap&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;std::find_if&lt;/code&gt; on &lt;code class=&quot;highlighter-rouge&quot;&gt;vector&amp;lt;pair&amp;lt;int, int&amp;gt;&amp;gt;&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;O(1)&lt;/td&gt;
      &lt;td&gt;O(n)&lt;/td&gt;
      &lt;td&gt;O(n)&lt;/td&gt;
      &lt;td&gt;This is a vector of pairs (Linear search needed)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;optimal-setup&quot;&gt;Optimal Setup&lt;/h4&gt;

&lt;p&gt;Initially, I want to gauge the lookup time for an ideal situation (ie where the cache is maximally effective). This can be easily accomplished by repeatedly searching for the same element so that the cache is always “hot”. Furthermore, the data structures will be initialized in a way that avoids dynamic reallocation and collisions, typically with the &lt;code class=&quot;highlighter-rouge&quot;&gt;reserve&lt;/code&gt; routine prior to benchmarking.&lt;/p&gt;

&lt;p&gt;First, I will define a benchmarking fixture which defines the collection and prevents repeated initialization of the containers (benchmarking is very slow without this). The below code is the Bench setup for the &lt;code class=&quot;highlighter-rouge&quot;&gt;std::unordered_map&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;UnorderedMapFixture&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fixture&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;public:&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SetUp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;State&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// preallocate space for n elements: controls the load factor of the map&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// populate the unordered_map&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TearDown&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;State&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/*state*/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Clear the unordered_map&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unordered_map&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr /&gt;

&lt;p&gt;Ok now that the underlying container has been initialized, a simple routine is written to poll the same arbitrary number from the container. This is automatically timed by the benchmark library with exponentially increasing sizes bounded by 8 * 2^24&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;n&quot;&gt;BENCHMARK_DEFINE_F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnorderedMapFixture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BM_UnorderedMapLookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;State&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  
  &lt;span class=&quot;c1&quot;&gt;// use n/2 to poll&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;benchmark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DoNotOptimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// Register the lookup benchmark with the fixture&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;BENCHMARK_REGISTER_F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UnorderedMapFixture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BM_UnorderedMapLookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;BENCHMARK_MAIN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p class=&quot;notice&quot;&gt;Note: &lt;code class=&quot;highlighter-rouge&quot;&gt;benchmark::DoNotOptimize&lt;/code&gt; is used to indicate to the compiler that it should not optimize away the lookup despite the value not being used.&lt;/p&gt;

&lt;p&gt;While executing this program, the benchmarking utility will determine the number of iterations required to build a high confidence estimate for the true average runtime. The timing results for each unordered_set size, including iterations, is shown in the console output snippet below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Running ./benchmark
Run on (36 X 3000 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x18)
  L1 Instruction 32 KiB (x18)
  L2 Unified 1024 KiB (x18)
  L3 Unified 25344 KiB (x1)
Load Average: 0.00, 0.08, 0.09
----------------------------------------------------------------------------------------------
Benchmark                                                    Time             CPU   Iterations
----------------------------------------------------------------------------------------------
UnorderedMapFixture/BM_UnorderedMapLookup/8               81.6 ns         81.6 ns      8589041
UnorderedMapFixture/BM_UnorderedMapLookup/64              81.6 ns         81.6 ns      8580608
UnorderedMapFixture/BM_UnorderedMapLookup/512             85.2 ns         85.2 ns      8590001
UnorderedMapFixture/BM_UnorderedMapLookup/4096            87.6 ns         87.6 ns      8000064
UnorderedMapFixture/BM_UnorderedMapLookup/32768           87.6 ns         87.6 ns      7997139
UnorderedMapFixture/BM_UnorderedMapLookup/262144          87.6 ns         87.6 ns      7874893
UnorderedMapFixture/BM_UnorderedMapLookup/2097152         81.6 ns         81.6 ns      8252177
UnorderedMapFixture/BM_UnorderedMapLookup/16777216        87.6 ns         87.6 ns      7988971
UnorderedMapFixture/BM_UnorderedMapLookup/134217728       88.0 ns         88.0 ns      7996966
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This procedure was repeated for all of the containers, and the plots are shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cache_bms/optimal.png&quot; alt=&quot;optimal plots&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These results look great! This behavior almost perfectly models the asympototic behavior of the functions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The unordered map has a O(1) runtime: It takes ~85ns to lookup an element at both 8 elements and 100 million elements&lt;/li&gt;
  &lt;li&gt;The map has a O(log(n)) runtime. This manifests as a linear plot when the x-axis is log scaled.&lt;/li&gt;
  &lt;li&gt;The vector has a O(n) runtime. This manifests as an exponential plot when the x-axis is log scaled.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;realistic-setup&quot;&gt;Realistic setup&lt;/h4&gt;

&lt;p&gt;Ok, so establishing the data structures to always make effective use of the cache clearly provides consistently excellent performance and highlights the effect of algorithmic complexity on the runtime. This occurs due to repeatedly accessing the same few memory regions: taking advantage of the principle of &lt;a href=&quot;https://www.sciencedirect.com/topics/computer-science/temporal-locality#:~:text=Temporal%20locality%20is%20the%20tendency,an%20appropriate%20data%2Dmanagement%20heuristic.&quot;&gt;temporal locality&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to create a more realistic situation with significantly more cache misses, the element to lookup will be randomly generated from a uniform distribution. This means the probability of a cache hit is approx:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cache_bms/eq1.png&quot; alt=&quot;hit eq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where B is the bytes in the cache and the data structure respectively&lt;/p&gt;

&lt;p&gt;This situation is likely bordering artificially inefficient, since it’s rare for memory accesses to be completely random, but it should still illustrate what happens when the size of the collection exceeds the bounds of a cache. In order to test under these new circumstances, the same benchmarking code will be used with the addition of &lt;a href=&quot;https://cplusplus.com/reference/random/mt19937/&quot;&gt;Random Number Generation&lt;/a&gt; for determining what number to poll in the collection.&lt;/p&gt;

&lt;p&gt;Before, the benchmarks are run I want to do some napkin math to estimate the number of elements at which the cache will become saturated.&lt;/p&gt;

&lt;p&gt;Since each k-v pair element contains 2 32-bit integers, the entire element will be 8 bytes. The L2 cache contains 1024 kb (per core), which is the most relevant due to the process running on a single core.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cache_bms/EQ2.png&quot; alt=&quot;saturated size&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the calculations above, we estimate each container will fully saturate the L2 cache at around 1.3E5 elements. In our plots this number will be added as a horizontal line.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cache_bms/realistic.png&quot; alt=&quot;realistic plots&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;These results are super interesting!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The good news is that our saturation point calculations seems to be spot on!&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For the Unordered map, the initial lookup time was hovering right around 120 ns until around 100k elements where the latency began to increase until it eventually settled at around 450ns as the cache became extremely ineffective.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;notice&quot;&gt;Note: The lookup time went up from around 85ns to 120ns due to the addition of computing a pseudo-random number. This is a constant offset and doesn’t effect the analysis&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Similarly for the map, the log scaled plot has an initially low slope which suddenly increases at right around the saturation point. The change in slope directly demonstrates the increase in access latency&lt;/li&gt;
  &lt;li&gt;This indicates that &lt;strong&gt;we can definitely predict the impact of CPU cache and create really solid estimates for runtime!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The bad news is that the vector lookup behavior seems completely unchanged. However, some research reveals that the underlying data for a &lt;code class=&quot;highlighter-rouge&quot;&gt;pair&amp;lt;int, int&amp;gt;&lt;/code&gt; is allocated on the heap and the vector contains the pointer to the underlying heap region. This does explain why the saturation point isn’t a great model for point of declining performance. Making frequent accesses to the heap to dereference pointers may have had a large impact on the effectiveness of the cache’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Locality_of_reference&quot;&gt;spatial locality&lt;/a&gt; and the boundary between good and bad cache utilization is not as clear or significant as with the other two collections. This also brings to light a good lesson while exploring the impact of architecture on application performance: &lt;strong&gt;Sometimes it gets complicated to exactly predict performance due to the sheer number of factors at play&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for giving this a read. I’d love to hear your thoughts so feel free to shoot me an email!&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;tl;dr&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;There are a lot of different kinds of memory. The fast stuff is expensive, so we need to use it well!&lt;/li&gt;
  &lt;li&gt;We use the fast stuff to design a special memory store between the CPU and main memory called cache&lt;/li&gt;
  &lt;li&gt;This cache can be designed in different ways each with it’s own tradeoffs&lt;/li&gt;
  &lt;li&gt;Modern processors have layers of different chained cache’s with varying properties.&lt;/li&gt;
  &lt;li&gt;If our access patterns are perfectly suited to the cache, then our algorithms will behave very similar to their average asymptotic runtime&lt;/li&gt;
  &lt;li&gt;The access pattern is usually not that great, which introduces interesting and sometimes predictable changes to our performance&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Maanav Singh</name><email>msingh2@unc.edu</email></author><category term="systems" /><category term="embedded" /><summary type="html">Memory access times play a huge role on performance characteristics of programs. Here I conduct a set of experiments to isolate and understand these effects.</summary></entry><entry><title type="html">Rucket: Customizable In-Order Reliable Data Transport</title><link href="http://localhost:4000/systems/networking/2023/05/22/rucket.html" rel="alternate" type="text/html" title="Rucket: Customizable In-Order Reliable Data Transport" /><published>2023-05-22T15:23:54-04:00</published><updated>2023-05-22T15:23:54-04:00</updated><id>http://localhost:4000/systems/networking/2023/05/22/rucket</id><content type="html" xml:base="http://localhost:4000/systems/networking/2023/05/22/rucket.html">&lt;p&gt;Reliable and fast data transport is hard. Let’s see if I can make it any easier&lt;/p&gt;

&lt;h2 id=&quot;whats-there-to-improve&quot;&gt;What’s there to improve?&lt;/h2&gt;

&lt;p&gt;In the relatively stagnant world of transport protocols, I often find myself grappling with a challenging dilemma: choosing between reliable data transport (RDT), transmission speed, and ease of development. While the tried-and-true options of Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) have their respective strengths, they also come with inherent limitations.&lt;/p&gt;

&lt;p&gt;While developing an application that requires inter-application communication, three prevailing options exist:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TCP ensures reliable delivery of data but can suffer from increased latency and overhead.&lt;/li&gt;
  &lt;li&gt;UDP offers blazing fast transmission speeds but lacks built-in reliability mechanisms.&lt;/li&gt;
  &lt;li&gt;A custom application layer transport scheme can provide the optimal balance but requires extensive domain knowledge and engineering effort&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-can-this-be-made-better&quot;&gt;How can this be made better?&lt;/h2&gt;

&lt;p&gt;Currently, if reliable in-order data transfer is required, most applications will simply utilize an OS syscall or language specific STL library to transport data using TCP. This approach is often a sore-spot for performance critical applications, where lengthy and non-modular measures are required to achieve reliability and performance requirements.&lt;/p&gt;

&lt;p&gt;My solution is to introduce an application layer library which abstracts away all the messy details of in-order reliable data transport, while also providing a simple interface for configuration: enabling the potential for superior performance.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I’m going to call this package &lt;strong&gt;Rucket&lt;/strong&gt;: Rust + {socket, packet}?&lt;/p&gt;

  &lt;p&gt;Which as you might have guessed, means that I will be writing the program with &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt;, but the target platform will be available in both Rust and &lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt;. This Rust to Python binding will be achieved using &lt;a href=&quot;https://github.com/PyO3/pyo3&quot;&gt;PyO3&lt;/a&gt;.&lt;/p&gt;

  &lt;p&gt;Some reasons why I think rust is a good fit for this project:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;It’s a low-level compiled language built on the &lt;a href=&quot;https://llvm.org/&quot;&gt;LLVM&lt;/a&gt; backend, and has been shown to generate very performant binaries&lt;/li&gt;
    &lt;li&gt;Very memory safe&lt;strong&gt;*&lt;/strong&gt; (without the need for a garbage collector). This will hopefully help stop me from introducing too many bugs!&lt;/li&gt;
    &lt;li&gt;I haven’t used rust for a large software project yet, so I wanted to give it a shot :3&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;check &lt;a href=&quot;https://visualstudiomagazine.com/articles/2019/07/18/microsoft-eyes-rust.aspx&quot;&gt;this&lt;/a&gt; neat article out!&lt;strong&gt;*&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/protocol_stack.png&quot; alt=&quot;Transport Stacks&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The above figure demonstrates a traditional hierarchy of protocols to reliably transmit data on the left vs. rucket’s newly proposed hierarchy on the right.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My guiding principle in developing this tool is:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Different applications have incredibly different transport requirements, so I want to make changing the behavior as easy as possible.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This idea leads to the following distinguishing features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Simple config interface for tweaking parameters&lt;/li&gt;
  &lt;li&gt;Nearly identical API to standard socket library&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;its-not-all-roses-&quot;&gt;It’s not all roses! 🌹&lt;/h2&gt;

&lt;p&gt;An under-appreciated but beautiful objective of TCP is to promote fair-sharing of network bandwidth. This avoids a debilitating  problem called &lt;a href=&quot;https://www.freesoft.org/CIE/RFC/896/2.htm&quot;&gt;Congestion Collapse&lt;/a&gt;: which is roughly analogous to “grid-lock” on a highway which prevents any vehicle’s passage. This prevention is achieved through an algorithm called Congestion Control: which probes the network for available bandwidth and promptly retracts usage once loss is encountered.&lt;/p&gt;

&lt;p&gt;Part of the speed improvement Rucket will be from loosening or disabling the parameters involved in encouraging fair sharing and hopefully saturating a larger fraction of the network’s throughput. Congestion control will still be available if the developer deems maintaining fair-sharing is necessary to prevent a drastic drop in performance from congestion.&lt;/p&gt;

&lt;p&gt;Furthermore, due to the requirement of TCP like data on top of existing UDP headers. There will likely be some overhead in usable capacity per segment. This loss is demonstrated in the below figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/segments.png&quot; alt=&quot;Segment diagrams&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This figure shows what a sample segment may look like for UDP, TCP, and Rucket respectively (top-&amp;gt;down). The red arrow indicated the data overhead of rucket over native TCP. But as demonstrated later, this difference will more than be compensated with increased throughput.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you’re looking for a simple and well supported way to reliably send data between instances of your applications, and you don’t have strict and well understood performance requirements I would wholly recommend clicking away to at least try something similar to &lt;a href=&quot;https://realpython.com/python-sockets/&quot;&gt;sockets&lt;/a&gt; or &lt;a href=&quot;https://requests.readthedocs.io/en/latest/&quot;&gt;requests&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This library should in-theory approximately match the performance of sockets with default configurations but with far less support. I hope to help you decide if the ability tune the internal implementation of transmission is worth this tradeoff&lt;/p&gt;

&lt;h2 id=&quot;cool-how-can-i-use-it&quot;&gt;Cool! How can I use it?&lt;/h2&gt;

&lt;p&gt;Rucket &lt;strong&gt;will soon&lt;/strong&gt; be available to use through python by installing the following &lt;a href=&quot;https://pypi.org/project/pip/&quot;&gt;PyPi package&lt;/a&gt;. The &lt;a href=&quot;https://github.com/MaanavS16&quot;&gt;source code&lt;/a&gt; is also available.&lt;/p&gt;

&lt;p&gt;It can be more simply installed using the following command&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install rucket
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you don’t care about the nitty gritty details, then the following code example should highlight the utility of the package.&lt;/p&gt;

&lt;h3 id=&quot;your-server&quot;&gt;Your server:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;rucket&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;SERVER_HOST&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;jn.server.net&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SERVER_PORT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6243&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# default configuration -- you can customize this
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Make sure to call rucket.socket.close() after use (implict w/ with keyword)
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# bind rucket&apos;s application to a specific port on the host
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SERVER_HOST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SERVER_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# blocking process, wait until someone connects
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;client_connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client_address&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# Now we can interact with client
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Block process until data arrives. Pulling at most 1024 bytes
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;client_message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client_connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I just got a message from &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client_address&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, saying &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;client_message&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;client_connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Thanks for the message :3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client_message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Goodbye&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;client_connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bye bye&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;client_connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;your-client&quot;&gt;Your client:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;rucket&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;MY_HOST&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;mv.client.net&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;MY_PORT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9103&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;socket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MY_HOST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MY_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;jn.server.net&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6243&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Wasup dawg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Goodbye&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;skt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This is a simple python program, that demonstrates how rucket can be used. The magic is really in the &lt;code class=&quot;highlighter-rouge&quot;&gt;rucket.config&lt;/code&gt; object which will specify the parameters and enable tweaking of how data is transmitted. Also note that this exchange is intentionally similar to the standard python &lt;a href=&quot;https://docs.python.org/3/library/socket.html&quot;&gt;sockets&lt;/a&gt; library to make migration easy.&lt;/p&gt;

&lt;h2 id=&quot;diving-into-the-details&quot;&gt;Diving into the details!&lt;/h2&gt;

&lt;p&gt;Ok, you’re still around, thanks!&lt;/p&gt;

&lt;p&gt;I’m still working on this part, so return to this page at at least one point in the interval $(now, \inf)$&lt;/p&gt;</content><author><name>Maanav Singh</name><email>msingh2@unc.edu</email></author><category term="systems" /><category term="networking" /><summary type="html">Reliable and fast data transport is hard. Let’s see if I can make it any easier</summary></entry></feed>