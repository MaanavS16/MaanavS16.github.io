<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Predicting effects of CPU Cache on Application Performance - Maanav Singh</title>
<meta name="description" content="Memory access times play a huge role on performance characteristics of programs. Here I conduct a set of experiments to isolate and understand these effects.">


  <meta name="author" content="Maanav Singh">
  
  <meta property="article:author" content="Maanav Singh">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Maanav Singh">
<meta property="og:title" content="Predicting effects of CPU Cache on Application Performance">
<meta property="og:url" content="http://localhost:4000/systems/embedded/2023/05/29/cache-exploration.html">


  <meta property="og:description" content="Memory access times play a huge role on performance characteristics of programs. Here I conduct a set of experiments to isolate and understand these effects.">







  <meta property="article:published_time" content="2023-05-29T15:23:54-04:00">






<link rel="canonical" href="http://localhost:4000/systems/embedded/2023/05/29/cache-exploration.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/",
      "sameAs": ["https://www.linkedin.com/in/maanav-singh/"]
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Maanav Singh Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Maanav Singh
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/index.html">About</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/">Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/projects.html">Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/education/">Teaching + Learning</a>
            </li><li class="masthead__menu-item">
              <a href="/assets/resume/maanav-singh-resume.pdf">Resume</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/re_li_pfp.png" alt="Maanav Singh" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Maanav Singh</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>CS and Math Student at UNC Chapel Hill</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Morrisville, NC</span>
        </li>
      

      

      

      
        <li>
          <a href="mailto:msingh2@unc.edu">
            <meta itemprop="email" content="msingh2@unc.edu" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/maanav-singh" itemprop="sameAs" rel="nofollow noopener noreferrer">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span>
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/maanavs16" itemprop="sameAs" rel="nofollow noopener noreferrer">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Predicting effects of CPU Cache on Application Performance">
    <meta itemprop="description" content="Memory access times play a huge role on performance characteristics of programs. Here I conduct a set of experiments to isolate and understand these effects.">
    <meta itemprop="datePublished" content="2023-05-29T15:23:54-04:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Predicting effects of CPU Cache on Application Performance
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Memory access times play a huge role on performance characteristics of programs. Here I conduct a set of experiments to isolate and understand these effects.</p>

<h2 id="introduction">Introduction</h2>
<p>Memory is absolutely essential in the field of computing. From an automata theory standpoint, it’s the key tool that enables us to reason with context-free and recursively enumerable languages. Beyond the theoretical power memory yields, the behavior of various memory models and techniques heavily influence modern software.</p>

<p>My goal with these experiments is to fill in some of the holes in my knowledge with some hands on analysis and some documentation I can reference in the future. Before diving into the details I’d like to do a bottom-up review of computer memory in general.</p>

<h2 id="types-of-memory">Types of Memory</h2>
<p>The basic concept of memory in computer is to create a physical representation for data that can be written to and read from. Pretty simple concept, but the physical design and fabrication is incredibly complicated. The most common forms of memory that you’ll encounter while working with computers are listed in the following table.</p>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Access Latency</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Static Random Access Memory (SRAM)</td>
      <td><strong>1-10 ns</strong></td>
      <td>Super fast and super expensive. Built with transistors + pretty much a <a href="https://www.build-electronic-circuits.com/d-latch/">D-Latch</a></td>
    </tr>
    <tr>
      <td>Dynamic Random Access Memory (DRAM)</td>
      <td><strong>~50 ns</strong></td>
      <td>Main memory implemented with this. Fast and expensive</td>
    </tr>
    <tr>
      <td>Flash Memory</td>
      <td><strong>1-100 µs</strong></td>
      <td>No moving components and still non-volatile</td>
    </tr>
    <tr>
      <td>Disk Storage</td>
      <td><strong>~10 ms</strong> w/ 7200 rpm</td>
      <td>Cheap spinning disks with magnetic platters and a read/write head that sets and specific magnetized regions.</td>
    </tr>
    <tr>
      <td>Magnetic Tape</td>
      <td><strong>seconds to hours</strong></td>
      <td>Used for archival storage. Extremely high capacity per unit cost. Cool <a href="https://archiveprogram.github.com/arctic-vault/">example</a></td>
    </tr>
  </tbody>
</table>

<p class="notice">Note: All of these access times are mostly for determining a general order of magnitude. At small-time scales the distance between the processor and the memory cells can play a significant role in access latency.</p>

<p><img src="/assets/images/MemoryHierarchy.png" alt="memory hierarchy" /></p>

<p>The above figure demonstrates the scarcity, speed, and cost of the memory types. Where the top of the pyramid has the least quantity, highest cost, and highest speed.</p>

<h2 id="cache-layout">Cache Layout:</h2>

<p>The Von Neumann architecture prescribes a unified memory store that both contains instructions and application data. While this is mostly true, as the system memory is unified, there exists a lower latency intermediate memory within the CPU that contains a portion of the memory space called the Cache. The CPU can often avoid making costly accesses to the main memory by taking advantage of this. The cache saves data entries that are likely to be accessed in the future and makes them available for quick access. When a particular memory addresses is trying to resolved, the cache is first checked and can potentially massively decrease access latency.</p>

<p>This cache differs from the main memory in more ways than just speed however. The cache is designed to determine what’s “likely to be accessed in the future”. The strategy for completing this determination varies between caches and depends on the access patterns and configuration.</p>

<p>There are two guiding observations that determine the high level design of caches.</p>
<ol>
  <li>If an address is accessed, then adjacent elements are also likely to be accessed (<a href="https://en.wikipedia.org/wiki/Locality_of_reference">Spatial Locality</a>)</li>
  <li>If an address is accessed, then that element is likely to be accessed in the future (<a href="https://en.wikipedia.org/wiki/Locality_of_reference">Temporal Locality</a>)</li>
</ol>

<p>I won’t go into much detail beyond these fairly straightforward definitions, but the influence of these two observations should hopefully be evident moving forward.</p>

<h3 id="should-instructions-and-data-be-cached-separately">Should instructions and data be cached separately?</h3>

<table>
  <thead>
    <tr>
      <th>Decision</th>
      <th>Pros</th>
      <th>Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Yes</td>
      <td>More hits per cache = more hits overall</td>
      <td>If predecessor cache’s use a separate store, then then the access patterns will make this less cache effective.</td>
    </tr>
    <tr>
      <td>No</td>
      <td>Simple and fast</td>
      <td>Cache trashing can occur: lines are being wasted by flip-flopping between instructions and data</td>
    </tr>
  </tbody>
</table>

<p>Due to the advantages of separately caching instructions and data, most modern processors have separate caches at the first level. However, subsequent layers are unified due to the diminishing performance returns with further separation on larger caches. The following diagram diagrams the cache in my current CPU. Notably, there are 3 stages to the cache.</p>

<ul>
  <li>L1i: Per core instruction cache + L1d: Per core data cache</li>
  <li>L2: Per core unified cache following L1</li>
  <li>L3: Across core unified cache</li>
</ul>

<p><img src="/assets/images/cpu_diagram.png" alt="cpu diagram" /></p>

<p>Now that we’ve covered the high level design of a cpu cache scheme, let’s focus on the specific design of each of the cache boxes in the diagram. There are 2 main types of cache strategies: Direct map and Set associative.</p>

<h3 id="direct-mapped-or-set-associative">Direct mapped or set associative?</h3>
<h4 id="direct-mapped">Direct mapped:</h4>

<p>Partition the memory space into n regions, where there are n lines in the cache (So each partition gets a single line in the physical cache). If n is a power of two, then you can use the upper-order bits of an address to identify what the corresponding cache line is.</p>

<p>Each cache contains multiple words, meaning a single memory access</p>

<p>Steps for memory lookup in a direct mapped cache:</p>

<ol>
  <li>CPU executes load instruction for a particular address.</li>
  <li>Set index portion of the address determines the relevant cache line</li>
  <li>(a) if the valid flag is set and the tag matches then the cache has hit. Return correct word by finding the offset within the data-field.</li>
  <li>(b) otherwise, issue a memory fetch from upper level cache/memory. After that’s resolved, evict the data and update the data field, tag, and set the valid bit.</li>
</ol>

<p><img src="/assets/images/Block-diagram-of-a-direct-mapped-cache.png" alt="dmcache" /></p>

<h4 id="set-associative">Set Associative:</h4>

<p>A set associative cache is similar to a stack of smaller direct mapped caches. That’s to say, each partition of the memory space has k corresponding caches lines across the k stacked direct mapped caches. This type of cache is called a k-way set associative cache.</p>

<p>Now that there are more than one cache lines for a particular set, more logic needs to be implemented for deciding which entires from the set get to stay and which get evicted from the cache. In order to implement this behavior, there are a few bits in the cache line to store information about that particular entry.</p>

<p>These bits are often used to store either frequency or access time information. With that the eviction strategy can be to remove the least frequently accessed data region or the last accessed data region. (LFU/LRU caches)</p>

<p>Steps for memory lookup in a direct mapped cache:</p>

<ol>
  <li>CPU executes load instruction for a particular address.</li>
  <li>Set index portion of the address determines the relevant cache lines</li>
  <li>(a) If the valid flag is set and the tag matches in any of the cache lines, return correct word by finding the offset within the data-field. Update the frequency or access time bits if applicable</li>
  <li>(b) otherwise, issue a memory fetch from upper level cache/memory. After, determine which line should be evicted to make space if any.</li>
</ol>

<p><img src="/assets/images/sacache.jpg" alt="sacache" /></p>

<p>A set associative cache is more advanced than a direct map cache, which results in more intelligent cache eviction and consequently higher hit rates. However, this comes at the cost of complexity and it’s associated latency increases.</p>

<p>average_latency = hit% * (cache access latency) + (1-hit%) * (memory access latency)</p>

<p>Where, hit% is determined by the cache type, strategy, and access patterns
and cache access latency is determined by the design and complexity of the cache.</p>

<p>This overview of the operating of direct map and set associative caches neglects to review the procedure for writing to memory, since the focus on the following sections is read performance; however, it’s important to understand there are mechanisms for marking cache entries as unusable or “dirty” after they’ve been overwritten or modified. There are also challenges with <a href="https://en.wikipedia.org/wiki/Cache_coherence">Cache Coherence</a> or making sure that all caches contain correct data even across different CPU cores. This won’t be explored here, but is definitely worth reading into.</p>

<h2 id="jumping-into-some-code">Jumping into some code</h2>

<p>After the introduction to basic cache structures, I’ve set 2 tasks for myself.</p>

<ol>
  <li>Try to make a perfect situation for the cache to shine in terms of performance.</li>
  <li>Explore the impact of a bad access pattern on performance, and try to model and predict that impact.</li>
</ol>

<div class="notice">
<p>All following benchmarks will be run w/</p>
<ul>
  <li>i9-10980XE processor</li>
  <li>2x32 GB DDR4 Memory @ 2666 MHZ</li>
  <li>gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1)</li>
  <li><a href="https://github.com/google/benchmark">google benchmarking library</a></li>
</ul>
</div>

<h3 id="c-stl-containers">C++ STL Containers</h3>

<p>I will be using various C++ standard STL containers to create a key-value store as my case study for cache performance. In a basic Data Structures setting, performance considerations for an algorithm are solely based on the asymptotic runtime behavior. This is shown to be a very simplistic model and doesn’t convey much information about the runtime characteristics related to the host architecture.</p>

<p>In my tests, I will measure the lookup time for a particular integer key value pair across <code class="highlighter-rouge">std::vector&lt;std::pair&lt;int, int&gt;&gt;</code>, <code class="highlighter-rouge">std::map&lt;int, int&gt;</code>, <code class="highlighter-rouge">std::unordered_map&lt;int, int&gt;</code>. Each of these collections will be initialized with ascending keys and a matching 32 bit integer (8 bytes per element).</p>

<p>As a quick reminder, these are the following best, average, and worst case lookup algorithm time complexities.</p>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th>Best</th>
      <th>Average</th>
      <th>Worst</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">std::unordered_map::find</code></td>
      <td>O(1)</td>
      <td>O(1)</td>
      <td>O(1)</td>
      <td>This is a hashmap</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">std::map::find</code></td>
      <td>O(log(n))</td>
      <td>O(log(n))</td>
      <td>O(log(n))</td>
      <td>This is a treemap</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">std::find_if</code> on <code class="highlighter-rouge">vector&lt;pair&lt;int, int&gt;&gt;</code></td>
      <td>O(1)</td>
      <td>O(n)</td>
      <td>O(n)</td>
      <td>This is a vector of pairs (Linear search needed)</td>
    </tr>
  </tbody>
</table>

<h4 id="optimal-setup">Optimal Setup</h4>

<p>Initially, I want to gauge the lookup time for an ideal situation (ie where the cache is maximally effective). This can be easily accomplished by repeatedly searching for the same element so that the cache is always “hot”. Furthermore, the data structures will be initialized in a way that avoids dynamic reallocation and collisions, typically with the <code class="highlighter-rouge">reserve</code> routine prior to benchmarking.</p>

<p>First, I will define a benchmarking fixture which defines the collection and prevents repeated initialization of the containers (benchmarking is very slow without this). The below code is the Bench setup for the <code class="highlighter-rouge">std::unordered_map</code>.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">class</span> <span class="nc">UnorderedMapFixture</span> <span class="o">:</span> <span class="k">public</span> <span class="n">benchmark</span><span class="o">::</span><span class="n">Fixture</span> <span class="p">{</span>
<span class="nl">public:</span>
  <span class="kt">void</span> <span class="n">SetUp</span><span class="p">(</span><span class="k">const</span> <span class="o">::</span><span class="n">benchmark</span><span class="o">::</span><span class="n">State</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// preallocate space for n elements: controls the load factor of the map</span>
    <span class="n">map</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>

    <span class="c1">// populate the unordered_map</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">state</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
      <span class="n">map</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>

  <span class="kt">void</span> <span class="n">TearDown</span><span class="p">(</span><span class="k">const</span> <span class="o">::</span><span class="n">benchmark</span><span class="o">::</span><span class="n">State</span><span class="o">&amp;</span> <span class="cm">/*state*/</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Clear the unordered_map</span>
    <span class="n">map</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">map</span><span class="p">;</span>
<span class="p">};</span></code></pre></figure>

<hr />

<p>Ok now that the underlying container has been initialized, a simple routine is written to poll the same arbitrary number from the container. This is automatically timed by the benchmark library with exponentially increasing sizes bounded by 8 * 2^24</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">BENCHMARK_DEFINE_F</span><span class="p">(</span><span class="n">UnorderedMapFixture</span><span class="p">,</span> <span class="n">BM_UnorderedMapLookup</span><span class="p">)(</span><span class="n">benchmark</span><span class="o">::</span><span class="n">State</span><span class="o">&amp;</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
  
  <span class="c1">// use n/2 to poll</span>
  <span class="kt">int</span> <span class="n">key</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>

  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">_</span> <span class="o">:</span> <span class="n">state</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">it</span> <span class="o">=</span> <span class="n">map</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">key</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">it</span> <span class="o">!=</span> <span class="n">map</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="n">it</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">;</span>
      <span class="n">benchmark</span><span class="o">::</span><span class="n">DoNotOptimize</span><span class="p">(</span><span class="n">value</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="c1">// Register the lookup benchmark with the fixture</span>
<span class="n">BENCHMARK_REGISTER_F</span><span class="p">(</span><span class="n">UnorderedMapFixture</span><span class="p">,</span> <span class="n">BM_UnorderedMapLookup</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span> <span class="o">&lt;&lt;</span> <span class="mi">24</span><span class="p">);</span>
<span class="n">BENCHMARK_MAIN</span><span class="p">();</span></code></pre></figure>

<p class="notice">Note: <code class="highlighter-rouge">benchmark::DoNotOptimize</code> is used to indicate to the compiler that it should not optimize away the lookup despite the value not being used.</p>

<p>While executing this program, the benchmarking utility will determine the number of iterations required to build a high confidence estimate for the true average runtime. The timing results for each unordered_set size, including iterations, is shown in the console output snippet below.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Running ./benchmark
Run on (36 X 3000 MHz CPU s)
CPU Caches:
  L1 Data 32 KiB (x18)
  L1 Instruction 32 KiB (x18)
  L2 Unified 1024 KiB (x18)
  L3 Unified 25344 KiB (x1)
Load Average: 0.00, 0.08, 0.09
----------------------------------------------------------------------------------------------
Benchmark                                                    Time             CPU   Iterations
----------------------------------------------------------------------------------------------
UnorderedMapFixture/BM_UnorderedMapLookup/8               81.6 ns         81.6 ns      8589041
UnorderedMapFixture/BM_UnorderedMapLookup/64              81.6 ns         81.6 ns      8580608
UnorderedMapFixture/BM_UnorderedMapLookup/512             85.2 ns         85.2 ns      8590001
UnorderedMapFixture/BM_UnorderedMapLookup/4096            87.6 ns         87.6 ns      8000064
UnorderedMapFixture/BM_UnorderedMapLookup/32768           87.6 ns         87.6 ns      7997139
UnorderedMapFixture/BM_UnorderedMapLookup/262144          87.6 ns         87.6 ns      7874893
UnorderedMapFixture/BM_UnorderedMapLookup/2097152         81.6 ns         81.6 ns      8252177
UnorderedMapFixture/BM_UnorderedMapLookup/16777216        87.6 ns         87.6 ns      7988971
UnorderedMapFixture/BM_UnorderedMapLookup/134217728       88.0 ns         88.0 ns      7996966
</code></pre></div></div>

<p>This procedure was repeated for all of the containers, and the plots are shown below:</p>

<p><img src="/assets/images/cache_bms/optimal.png" alt="optimal plots" /></p>

<p>These results look great! This behavior almost perfectly models the asympototic behavior of the functions:</p>
<ul>
  <li>The unordered map has a O(1) runtime: It takes ~85ns to lookup an element at both 8 elements and 100 million elements</li>
  <li>The map has a O(log(n)) runtime. This manifests as a linear plot when the x-axis is log scaled.</li>
  <li>The vector has a O(n) runtime. This manifests as an exponential plot when the x-axis is log scaled.</li>
</ul>

<hr />
<h4 id="realistic-setup">Realistic setup</h4>

<p>Ok, so establishing the data structures to always make effective use of the cache clearly provides consistently excellent performance and highlights the effect of algorithmic complexity on the runtime. This occurs due to repeatedly accessing the same few memory regions: taking advantage of the principle of <a href="https://www.sciencedirect.com/topics/computer-science/temporal-locality#:~:text=Temporal%20locality%20is%20the%20tendency,an%20appropriate%20data%2Dmanagement%20heuristic.">temporal locality</a>.</p>

<p>In order to create a more realistic situation with significantly more cache misses, the element to lookup will be randomly generated from a uniform distribution. This means the probability of a cache hit is approx:</p>

<p><img src="/assets/images/cache_bms/eq1.png" alt="hit eq" /></p>

<p>Where B is the bytes in the cache and the data structure respectively</p>

<p>This situation is likely bordering artificially inefficient, since it’s rare for memory accesses to be completely random, but it should still illustrate what happens when the size of the collection exceeds the bounds of a cache. In order to test under these new circumstances, the same benchmarking code will be used with the addition of <a href="https://cplusplus.com/reference/random/mt19937/">Random Number Generation</a> for determining what number to poll in the collection.</p>

<p>Before, the benchmarks are run I want to do some napkin math to estimate the number of elements at which the cache will become saturated.</p>

<p>Since each k-v pair element contains 2 32-bit integers, the entire element will be 8 bytes. The L2 cache contains 1024 kb (per core), which is the most relevant due to the process running on a single core.</p>

<p><img src="/assets/images/cache_bms/EQ2.png" alt="saturated size" /></p>

<p>Given the calculations above, we estimate each container will fully saturate the L2 cache at around 1.3E5 elements. In our plots this number will be added as a horizontal line.</p>

<p><img src="/assets/images/cache_bms/realistic.png" alt="realistic plots" /></p>

<p><em>These results are super interesting!</em></p>

<p>The good news is that our saturation point calculations seems to be spot on!</p>
<ul>
  <li>For the Unordered map, the initial lookup time was hovering right around 120 ns until around 100k elements where the latency began to increase until it eventually settled at around 450ns as the cache became extremely ineffective.</li>
</ul>

<p class="notice">Note: The lookup time went up from around 85ns to 120ns due to the addition of computing a pseudo-random number. This is a constant offset and doesn’t effect the analysis</p>

<ul>
  <li>Similarly for the map, the log scaled plot has an initially low slope which suddenly increases at right around the saturation point. The change in slope directly demonstrates the increase in access latency</li>
  <li>This indicates that <strong>we can definitely predict the impact of CPU cache and create really solid estimates for runtime!</strong></li>
</ul>

<p>The bad news is that the vector lookup behavior seems completely unchanged. However, some research reveals that the underlying data for a <code class="highlighter-rouge">pair&lt;int, int&gt;</code> is allocated on the heap and the vector contains the pointer to the underlying heap region. This does explain why the saturation point isn’t a great model for point of declining performance. Making frequent accesses to the heap to dereference pointers may have had a large impact on the effectiveness of the cache’s <a href="https://en.wikipedia.org/wiki/Locality_of_reference">spatial locality</a> and the boundary between good and bad cache utilization is not as clear or significant as with the other two collections. This also brings to light a good lesson while exploring the impact of architecture on application performance: <strong>Sometimes it gets complicated to exactly predict performance due to the sheer number of factors at play</strong>.</p>

<p>Thanks for giving this a read. I’d love to hear your thoughts so feel free to shoot me an email!</p>

<h2 id="tldr">tl;dr</h2>

<ul>
  <li>There are a lot of different kinds of memory. The fast stuff is expensive, so we need to use it well!</li>
  <li>We use the fast stuff to design a special memory store between the CPU and main memory called cache</li>
  <li>This cache can be designed in different ways each with it’s own tradeoffs</li>
  <li>Modern processors have layers of different chained cache’s with varying properties.</li>
  <li>If our access patterns are perfectly suited to the cache, then our algorithms will behave very similar to their average asymptotic runtime</li>
  <li>The access pattern is usually not that great, which introduces interesting and sometimes predictable changes to our performance</li>
</ul>


        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-05-29T15:23:54-04:00">May 29, 2023</time></p>


      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=Predicting+effects+of+CPU+Cache+on+Application+Performance%20http%3A%2F%2Flocalhost%3A4000%2Fsystems%2Fembedded%2F2023%2F05%2F29%2Fcache-exploration.html" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fsystems%2Fembedded%2F2023%2F05%2F29%2Fcache-exploration.html" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fsystems%2Fembedded%2F2023%2F05%2F29%2Fcache-exploration.html" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/systems/networking/2023/05/22/rucket.html" class="pagination--pager" title="Rucket: Customizable In-Order Reliable Data Transport
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/systems/networking/2023/05/22/rucket.html" rel="permalink">Rucket: Customizable In-Order Reliable Data Transport
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Reliable and fast data transport is hard. Let’s see if I can make it any easier

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Maanav Singh. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
